{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In unsupervised learning, the task is to infer hidden structure from\n",
    "unlabeled data, comprised of training examples $\\{x_n\\}$.\n",
    "\n",
    "We demonstrate with an example in Edward. A webpage version is available at\n",
    "http://edwardlib.org/tutorials/unsupervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.distributions import Mixture\n",
    "\n",
    "from edward.models import (\n",
    "    Categorical, Dirichlet, Empirical, InverseGamma,\n",
    "    MultivariateNormalDiag, Normal, ParamMixture, PointMass, Mixture)\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Use a simulated data set of 2-dimensional data points\n",
    "$\\mathbf{x}_n\\in\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N):\n",
    "  pi = np.array([0.4, 0.3, 0.3])\n",
    "  mus = [[1, 1, 1], [-1, -1, -1], [-1,0, 1]]\n",
    "  stds = [[0.1, 0.1, 0.1], [0.1, 0.1, 0.1], [0.1, 0.1, 0.1]]\n",
    "  x = np.zeros((N, 3), dtype=np.float32)\n",
    "  for n in range(N):\n",
    "    k = np.argmax(np.random.multinomial(1, pi))\n",
    "    x[n, :] = np.random.multivariate_normal(mus[k], np.diag(stds[k]))\n",
    "\n",
    "  return x\n",
    "\n",
    "opt = 'MAP'\n",
    "N = 3000  # number of data points\n",
    "K = 3  # number of components\n",
    "D = 3  # dimensionality of data\n",
    "# ed.set_seed(42)\n",
    "\n",
    "x_train = build_toy_dataset(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the generated data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import dataset\n",
    "import pickle\n",
    "# with open('abc.pkl', 'rb') as aha:\n",
    "#     data = pickle.load(aha)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], x_train[:,2])\n",
    "# plt.axis([-3, 3, -3, 3])\n",
    "plt.title(\"Simulated dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "A mixture model is a model typically used for clustering.\n",
    "It assigns a mixture component to each data point, and this mixture component\n",
    "determines the distribution that the data point is generated from. A\n",
    "mixture of Gaussians uses Gaussian distributions to generate this data\n",
    "(Bishop, 2006).\n",
    "\n",
    "For a set of $N$ data points,\n",
    "the likelihood of each observation $\\mathbf{x}_n$ is\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{x}_n \\mid \\pi, \\mu, \\sigma)\n",
    "  &=\n",
    "  \\sum_{k=1}^K \\pi_k \\, \\text{Normal}(\\mathbf{x}_n \\mid \\mu_k, \\sigma_k).\n",
    "\\end{align*}\n",
    "\n",
    "The latent variable $\\pi$ is a $K$-dimensional probability vector\n",
    "which mixes individual Gaussian distributions, each\n",
    "characterized by a mean $\\mu_k$ and standard deviation $\\sigma_k$.\n",
    "\n",
    "Define the prior on $\\pi\\in[0,1]$ such that $\\sum_{k=1}^K\\pi_k=1$ to be\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\pi)\n",
    "  &=\n",
    "  \\text{Dirichlet}(\\pi \\mid \\alpha \\mathbf{1}_{K})\n",
    "\\end{align*}\n",
    "\n",
    "for fixed $\\alpha=1$. Define the prior on each component $\\mathbf{\\mu}_k\\in\\mathbb{R}^D$ to be\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{\\mu}_k)\n",
    "  &=\n",
    "  \\text{Normal}(\\mathbf{\\mu}_k \\mid \\mathbf{0}, \\mathbf{I}).\n",
    "\\end{align*}\n",
    "\n",
    "Define the prior on each component $\\mathbf{\\sigma}_k^2\\in\\mathbb{R}^D$ to be\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{\\sigma}_k^2)\n",
    "  &=\n",
    "  \\text{InverseGamma}(\\mathbf{\\sigma}_k^2 \\mid a, b).\n",
    "\\end{align*}\n",
    "\n",
    "We build two versions of the model in Edward: one jointly with the\n",
    "mixture assignments $c_n\\in\\{0,\\ldots,K-1\\}$ as latent variables,\n",
    "and another with them summed out.\n",
    "\n",
    "The joint version includes an explicit latent variable for the mixture\n",
    "assignments. We implement this with the `ParamMixture` random\n",
    "variable; it takes as input the mixing probabilities, the components'\n",
    "parameters, and the distribution of the components. It is the\n",
    "distribution of the mixture conditional on mixture assignments. (Note\n",
    "we can also write this separately by first building a `Categorical`\n",
    "random variable for `z` and then building `x`; `ParamMixture` avoids\n",
    "requiring `tf.gather` which is slightly more efficient.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pi = Dirichlet(tf.ones(K))\n",
    "# mu = Normal(tf.zeros(D), tf.ones(D), sample_shape=K)\n",
    "# sigmasq = InverseGamma(tf.ones(D), tf.ones(D), sample_shape=K)\n",
    "# x = ParamMixture(pi, {'loc': mu, 'scale_diag': tf.sqrt(sigmasq)},\n",
    "#                  MultivariateNormalDiag,\n",
    "#                  sample_shape=N)\n",
    "# z = x.cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collapsed version marginalizes out the mixture assignments. We\n",
    "implement this with the `Mixture` random variable; it takes as\n",
    "input a Categorical distribution and a list of individual distribution\n",
    "components. It is the distribution of the mixture summing out the\n",
    "mixture assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi = Dirichlet(tf.ones(K))\n",
    "mu = Normal(tf.zeros(D), tf.ones(D), sample_shape=K)\n",
    "sigmasq = InverseGamma(tf.ones(D), tf.ones(D), sample_shape=K)\n",
    "cat = Categorical(probs=pi, sample_shape=N)\n",
    "components = [\n",
    "    MultivariateNormalDiag(mu[k], sigmasq[k], sample_shape=N)\n",
    "    for k in range(K)]\n",
    "x = Mixture(cat=cat, components=components,sample_shape=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the joint version in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Each distribution in the model is written with conjugate priors, so we\n",
    "can use Gibbs sampling. It performs Markov chain Monte Carlo by\n",
    "iterating over draws from the complete conditionals of each\n",
    "distribution, i.e., each distribution conditional on a previously\n",
    "drawn value. First we set up Empirical random variables which will\n",
    "approximate the posteriors using the collection of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if opt == 'mcmc':\n",
    "    T = 1000  # number of MCMC samples\n",
    "    qpi = Empirical(tf.Variable(tf.ones([T, K]) / K))\n",
    "    qmu = Empirical(tf.Variable(tf.zeros([T, K, D])))\n",
    "    qsigmasq = Empirical(tf.Variable(tf.ones([T, K, D])))\n",
    "#     qz = Empirical(tf.Variable(tf.zeros([T, N], dtype=tf.int32)))\n",
    "elif opt == 'MAP':\n",
    "#     qz = PointMass(params=tf.Variable(tf.random_normal([N,1])))\n",
    "    qmu = PointMass(params=tf.Variable(tf.random_normal([K, D])))\n",
    "    qsigmasq = PointMass(params=tf.Variable(tf.ones([K,D])))\n",
    "    qpi = PointMass(params=tf.Variable(tf.ones(K) / K))\n",
    "    inference = ed.MAP({mu:qmu, sigmasq:qsigmasq}, data={x: x_train})\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "    inference.run(n_iter=2000, n_print=100, optimizer=optimizer)\n",
    "elif opt == 'EM':\n",
    "    T = 1000\n",
    "#     qz = Empirical(tf.Variable(tf.zeros([T, N], dtype=tf.int32)))\n",
    "    qmu = PointMass(params=tf.Variable(tf.random_normal([K, D])))\n",
    "    qsigmasq = PointMass(params=tf.Variable(tf.ones([K,D])))\n",
    "    qpi = PointMass(params=tf.Variable(tf.ones(K) / K))\n",
    "    inference_m = ed.Gibbs({z: qz}, data={x: x_train, mu:qmu, pi:qpi, sigmasq:qsigmasq})\n",
    "    t_ph = tf.placeholder(tf.int32, [])\n",
    "    running_cluster_means = tf.reduce_mean(qmu.params[:t_ph], 0)\n",
    "    inference_e = ed.MAP({mu:qmu, pi:qpi, sigmasq:qsigmasq}, data={x: x_train, z:qz})\n",
    "    inference_e.initialize(optimizer = tf.train.AdamOptimizer(learning_rate=1e-3))\n",
    "    inference_m.initialize()\n",
    "    init = tf.global_variables_initializer()\n",
    "    init.run()\n",
    "    for i in range(inference_m.n_iter):\n",
    "        info_dict_m = inference_m.update()\n",
    "        info_dict_e = inference_e.update()\n",
    "        inference_e.print_progress(info_dict_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Gibbs sampling. We write the training loop explicitly, so that we can track\n",
    "the cluster means as the sampler progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if opt == 'mcmc':\n",
    "    inference = ed.Gibbs({pi: qpi, mu: qmu, sigmasq: qsigmasq},\n",
    "                         data={x: x_train})\n",
    "    inference.initialize()\n",
    "\n",
    "    sess = ed.get_session()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    t_ph = tf.placeholder(tf.int32, [])\n",
    "    running_cluster_means = tf.reduce_mean(qmu.params[:t_ph], 0)\n",
    "\n",
    "    for _ in range(inference.n_iter):\n",
    "      info_dict = inference.update()\n",
    "      inference.print_progress(info_dict)\n",
    "      t = info_dict['t']\n",
    "      if t % inference.n_print == 0:\n",
    "        print(\"\\nInferred cluster means:\")\n",
    "        print(sess.run(running_cluster_means, {t_ph: t - 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism\n",
    "\n",
    "We visualize the predicted memberships of each data point. We pick\n",
    "the cluster assignment which produces the highest posterior predictive\n",
    "density for each data point.\n",
    "\n",
    "To do this, we first draw a sample from the posterior and calculate a\n",
    "a $N\\times K$ matrix of log-likelihoods, one for each data point\n",
    "$\\mathbf{x}_n$ and cluster assignment $k$.\n",
    "We perform this averaged over 100 posterior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if opt == 'mcmc':\n",
    "    # Calculate likelihood for each data point and cluster assignment,\n",
    "    # averaged over many posterior samples. ``x_post`` has shape (N, 100, K, D).\n",
    "    mu_sample = qmu.sample(100)\n",
    "    sigmasq_sample = qsigmasq.sample(100)\n",
    "    x_post = Normal(loc=tf.ones([N, 1, 1, 1]) * mu_sample,\n",
    "                    scale=tf.ones([N, 1, 1, 1]) * tf.sqrt(sigmasq_sample))\n",
    "    x_broadcasted = tf.tile(tf.reshape(x_train, [N, 1, 1, D]), [1, 100, K, 1])\n",
    "\n",
    "    # Sum over latent dimension, then average over posterior samples.\n",
    "    # ``log_liks`` ends up with shape (N, K).\n",
    "    log_liks = x_post.log_prob(x_broadcasted)\n",
    "    log_liks = tf.reduce_sum(log_liks, 3)\n",
    "    log_liks = tf.reduce_mean(log_liks, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take the $\\arg\\max$ along the columns (cluster assignments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if opt == 'mcmc':\n",
    "    # Choose the cluster with the highest likelihood for each data point.\n",
    "    clusters = tf.argmax(log_liks, 1).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data points, colored by their predicted membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if opt == 'mcmc':\n",
    "    plt.scatter(x_train[:, 0], x_train[:, 1], c=clusters, cmap=cm.bwr)\n",
    "    plt.axis([-3, 3, -3, 3])\n",
    "    plt.title(\"Predicted cluster assignments\")\n",
    "    plt.show()\n",
    "if opt in ['MAP','EM']:\n",
    "    sess = ed.get_session()\n",
    "    zproto = sess.run(qmu.params)\n",
    "    print(zproto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has correctly clustered the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks: The log-sum-exp trick\n",
    "\n",
    "For a collapsed mixture model, implementing the log density can be tricky.\n",
    "In general, the log density is\n",
    "\n",
    "\\begin{align*}\n",
    "  \\log p(\\pi) +\n",
    "  \\Big[ \\sum_{k=1}^K \\log p(\\mathbf{\\mu}_k) + \\log\n",
    "  p(\\mathbf{\\sigma}_k) \\Big] +\n",
    "  \\sum_{n=1}^N \\log p(\\mathbf{x}_n \\mid \\pi, \\mu, \\sigma),\n",
    "\\end{align*}\n",
    "\n",
    "where the likelihood is\n",
    "\n",
    "\\begin{align*}\n",
    "  \\sum_{n=1}^N \\log p(\\mathbf{x}_n \\mid \\pi, \\mu, \\sigma)\n",
    "  &=\n",
    "  \\sum_{n=1}^N \\log \\sum_{k=1}^K \\pi_k \\, \\text{Normal}(\\mathbf{x}_n \\mid\n",
    "  \\mu_k, \\sigma_k).\n",
    "\\end{align*}\n",
    "\n",
    "To prevent numerical instability, we'd like to work on the log-scale,\n",
    "\n",
    "\\begin{align*}\n",
    "  \\sum_{n=1}^N \\log p(\\mathbf{x}_n \\mid \\pi, \\mu, \\sigma)\n",
    "  &=\n",
    "  \\sum_{n=1}^N \\log \\sum_{k=1}^K \\exp\\Big(\n",
    "  \\log \\pi_k + \\log \\text{Normal}(\\mathbf{x}_n \\mid \\mu_k, \\sigma_k)\\Big).\n",
    "\\end{align*}\n",
    "\n",
    "This expression involves a log sum exp operation, which is\n",
    "numerically unstable as exponentiation will often lead to one value\n",
    "dominating the rest. Therefore we use the log-sum-exp trick.\n",
    "It is based on the identity\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathbf{x}_{\\mathrm{max}}\n",
    "  &=\n",
    "  \\arg\\max \\mathbf{x},\n",
    "  \\\\\n",
    "  \\log \\sum_i \\exp(\\mathbf{x}_i)\n",
    "  &=\n",
    "  \\log \\Big(\\exp(\\mathbf{x}_{\\mathrm{max}}) \\sum_i \\exp(\\mathbf{x}_i -\n",
    "  \\mathbf{x}_{\\mathrm{max}})\\Big)\n",
    "  \\\\\n",
    "  &=\n",
    "  \\mathbf{x}_{\\mathrm{max}} + \\log \\sum_i \\exp(\\mathbf{x}_i -\n",
    "  \\mathbf{x}_{\\mathrm{max}}).\n",
    "\\end{align*}\n",
    "\n",
    "Subtracting the maximum value before taking the log-sum-exp leads to\n",
    "more numerically stable output. The $\\texttt{Mixture}$ random variable\n",
    "implements this trick for calculating the log-density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
